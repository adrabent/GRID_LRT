{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID_LRT Testbed Notebook\n",
    "\n",
    "## 1. Setting up the Environment\n",
    "\n",
    "The GRID LOFAR TOOLS have several infrastructure requirements. They are as follows:\n",
    "\n",
    "1. ASTRON LOFAR staging credentials\n",
    "2. PiCaS database access\n",
    "3. Valid GRID proxy\n",
    "\n",
    "\n",
    "Here, we'll test that all of the above are enabled and work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/apmechev/software/lib/python2.6/site-packages/GRID_LRT-0.2-py2.6.egg/GRID_LRT/__init__.pyc\n",
      "2018-01-29 14:23:44.987982 stager_access: Parsing user credentials from /home/apmechev/.awe/Environment.cfg\n",
      "2018-01-29 14:23:44.988129 stager_access: Creating proxy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import GRID_LRT\n",
    "print(GRID_LRT.__file__)\n",
    "import subprocess\n",
    "from GRID_LRT.get_picas_credentials import picas_cred\n",
    "from GRID_LRT.Staging import stage_all_LTA\n",
    "from GRID_LRT.Staging import state_all\n",
    "from GRID_LRT.Staging import stager_access\n",
    "from GRID_LRT.Staging.srmlist import srmlist\n",
    "from GRID_LRT import Token\n",
    "pc=picas_cred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give a confirmation of that your LOFAR ASTRON credentials were properly read:\n",
    "\n",
    "`2017-12-04 17:15:29.097902 stager_access: Parsing user credentials from /home/apmechev/.awe/Environment.cfg\n",
    "2017-12-04 17:15:29.097973 stager_access: Creating proxy`\n",
    "\n",
    "Next, we check that your PiCaS User and Database are set properly. You can also verify your password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pc.user)\n",
    "print(pc.database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the test srm.txt to show off our staging chops:\n",
    "\n",
    "Stage the test srm.txt file. You'll get a StageID that you can use later.\n",
    "\n",
    "## 2. Staging files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_srm_file='/home/apmechev/t/GRID_LRT/GRID_LRT/tests/srm_50_sara.txt'\n",
    "\n",
    "os.path.exists(test_srm_file)\n",
    "with open(test_srm_file,'r') as f:\n",
    "    file_contents = f.read()\n",
    "    print(file_contents.split()[0:3]) \n",
    "stageID=stage_all_LTA.main(test_srm_file) # NOTE! You (oll get two emails every time you do this!\n",
    "print(stageID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now re-run the cell below to check the current status of your staging request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(stage_all_LTA.get_stage_status(stageID)) #crashes (py2.7?)\n",
    "#The code below can also show you a more detailed status\n",
    "statuses=stager_access.get_progress()\n",
    "\n",
    "print(statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statuses=stage_all_LTA.get_stage_status(stageID)\n",
    "## When the staging completes, your stageID magically disappears from the database\n",
    "# Neat, huh?\n",
    "if not statuses:\n",
    "    print(\"Staging status no longer in LTA Database\") #This happens because bad programming\n",
    "else:\n",
    "    print(\"Staging request \"+str(stageID)+\" has status: \"+str(statuses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the status of the srms two different ways (with srmls and with gfal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(state_all.__file__)\n",
    "staged_status = state_all.main(test_srm_file) #Only works for Sara and Poznan files!\n",
    "\n",
    "#You can also supress the printing of statuses\n",
    "staged_status1 = state_all.main(test_srm_file, printout=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. srm lists:\n",
    "\n",
    "A dedicated class exists to handle lists of srmfiles. This class is a child of the python 'list' class and thus has all the capabilites of a list with some bells and whistles. \n",
    "\n",
    "It contains as properties the OBSID and LTA location of the files. \n",
    "\n",
    "Additionally, it can create generators that convert the srm:// links to gsiftp:// links, as well as staging links (Ones that can be fed into the state_all.py script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L229507\n",
      "sara\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "test_srm_file='/home/apmechev/t/GRID_LRT/GRID_LRT/tests/srm_50_sara.txt'\n",
    "\n",
    "s_list=srmlist() #Empty list of srms\n",
    "\n",
    "with open(test_srm_file,'r') as f:\n",
    "    for i in f.read().split():\n",
    "        s_list.append(i)\n",
    "print(s_list.OBSID)\n",
    "print(s_list.LTA_location)\n",
    "print(len(s_list)) #len works as with a normal list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above commands show that you can load a set of srm links into a srmlist object, and it will also hold the LTA location and the OBSID. Each srmlist object can hold only one OBSID and one LTA location, and makes checks on each append:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should return Different OBSID than previous items:\n",
      "Different OBSID than previous items\n"
     ]
    }
   ],
   "source": [
    "juelich_srm=str(\"srm://lofar-srm.fz-juelich.de:8443/pnfs/\"+\n",
    "\"fz-juelich.de/data/lofar/ops/projects/lc7_012/583139/L583139_SB000_uv.MS_900c9fcf.tar\")\n",
    "\n",
    "try:\n",
    "    s_list.append(juelich_srm)\n",
    "except AttributeError as e:\n",
    "    print(\"Should return Different OBSID than previous items:\\n\"+str(e))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create generators that transform all srm links into either gsiftp links (for use with globustools) or http links (for use with wget). Additionally gfal links can be made. These links can be used with the (old) staging scripts as well as to check the status of (sara and poznan) files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four different links for the same file:\n",
      "\n",
      "srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar\n",
      "\n",
      "gsiftp://gridftp.grid.sara.nl:2811/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar\n",
      "\n",
      "https://lofar-download.grid.sara.nl/lofigrid/SRMFifoGet.py?surl=srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar\n",
      "\n",
      "srm://srm.grid.sara.nl:8443/srm/managerv2?SFN=/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar\n"
     ]
    }
   ],
   "source": [
    "gsi_generator=s_list.gsi_links()\n",
    "\n",
    "g_list=[]\n",
    "for i in gsi_generator:\n",
    "    g_list.append(i)\n",
    "\n",
    "h_list=[]\n",
    "for i in s_list.http_links():\n",
    "    h_list.append(i)\n",
    "\n",
    "stage_list=[]\n",
    "for i in s_list.gfal_links():\n",
    "    stage_list.append(i)\n",
    "\n",
    "print(\"four different links for the same file:\")\n",
    "print(\"\")\n",
    "print(s_list[0]) \n",
    "print(\"\")\n",
    "\n",
    "print(g_list[-1]) #for some reason list is backwards??\n",
    "print(\"\")\n",
    "\n",
    "print(h_list[-1])\n",
    "print(\"\")\n",
    "\n",
    "print(stage_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gsiftp links are used with the globus-url-copy and uberftp -ls tools. \n",
    "\n",
    "The http links can be downloaded with wget\n",
    "\n",
    "the gfal links can be fed into the state_all script which returns the status of the files on the LTA.\n",
    "\n",
    "Finally: If you need to split your srmlist in a set of equally-sized chunks, this can be done with srmlist.slice_dicts. This is useful when creating jobs that run on multiple files at the same time (for example dppconcat, or even losoto steps!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['150', '140', '120', '130', '110', '100']\n",
      "\n",
      "d_10['140'] =\n",
      "['srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB140_uv.dppp.MS_a99ed735.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB141_uv.dppp.MS_568c1e9a.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB142_uv.dppp.MS_2a25f26f.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB143_uv.dppp.MS_11c5b025.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB144_uv.dppp.MS_ffadacf3.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB145_uv.dppp.MS_8a0a0359.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB146_uv.dppp.MS_9f12d505.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB147_uv.dppp.MS_6a966762.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB148_uv.dppp.MS_5d73b756.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB149_uv.dppp.MS_5418b56d.tar']\n",
      "\n",
      "d_10['150'] =\n",
      "['srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB150_uv.dppp.MS_f6fc7fc5.tar']\n",
      "\n",
      "type(d_10['100']) = <class 'GRID_LRT.Staging.srmlist.srmlist'>\n",
      "d_10['100'].OBSID = L229507\n",
      "['150', '100']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from GRID_LRT.Staging.srmlist import slice_dicts\n",
    "\n",
    "d_10=slice_dicts(s_list.sbn_dict())\n",
    "print(d_10.keys()) # Will show the 'names' of the chunks of 10  (the starting SB)\n",
    "print(\"\")\n",
    "\n",
    "print(\"d_10['140'] =\")\n",
    "print(d_10['140']) #10 srms here\n",
    "print(\"\")\n",
    "\n",
    "print(\"d_10['150'] =\")\n",
    "print(d_10['150']) #1 srm here\n",
    "print(\"\")\n",
    "\n",
    "print(\"type(d_10['100']) = \"+str(type(d_10['100']))) #the dict values are srmlist() themselves!\n",
    "print(\"d_10['100'].OBSID = \"+d_10['100'].OBSID)\n",
    "\n",
    "d_50=slice_dicts(s_list.sbn_dict(),50)\n",
    "print(d_50.keys()) # Will show the 'names' of the chunks of 50  (the starting SB)\\\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool can be used to batch create tokens such as in section 4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokens! \n",
    "### 4. a) The manual way\n",
    "\n",
    "Next we'll interface with PiCaS and start making tokens for our Observation:\n",
    "\n",
    "here we need a string to link all the tokens in one Observation. We'll use the string 'demo_'+username in the sksp_dev database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uname = os.environ['USER']\n",
    "th = Token.Token_Handler(t_type=\"jupyter_demo_\"+uname, uname=pc.user, pwd=pc.password, dbn='sksp_dev')\n",
    "\n",
    "#Create the overview_view (has the number of todo, done, error, running, [...] tokens)\n",
    "th.add_overview_view()\n",
    "\n",
    "#Add the satus views (By default 'todo', 'locked', 'done', 'error')\n",
    "th.add_status_views()\n",
    "\n",
    "#Manually create a token:\n",
    "manual_keys = {'manual_key':'manual_value','manual_int':1024}\n",
    "man_token_1 = th.create_token(keys=manual_keys, append=\"manual\") #will return the id of the manual token\n",
    "print('manual_token_ID = ' + man_token_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually create a Token with an automatic attachment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manual_keys = {'manual_key':'manual_value','manual_int':0}\n",
    "man_token_2 = th.create_token(keys=manual_keys, \n",
    "                            append=\"manual_with_attach\",\n",
    "                            attach=[open(test_srm_file),'srm_at_token_create.txt']) \n",
    "\n",
    "##We can also attach files after the token's been created:\n",
    "th.add_attachment(man_token_2, open(test_srm_file), 'srm_added_later.txt')\n",
    "\n",
    "#Double check that both files were attached. Returns a list of filenames:\n",
    "man_2_attachies = th.list_attachments(man_token_2)\n",
    "print(\"The two attached files are: \"+str(man_2_attachies))\n",
    "\n",
    "# We can also of course download attachments:\n",
    "saved_attach=th.get_attachment(man_token_2,man_2_attachies[0],savename=man_2_attachies[0])\n",
    "print(\"\")\n",
    "print('The attachemnt '+str(man_2_attachies[0])+\" was saved at \"+saved_attach)\n",
    "\n",
    "assert(os.path.exists(saved_attach))\n",
    "os.remove(saved_attach)\n",
    "assert(not os.path.exists(saved_attach))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also list the views and the tokens from each view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(th.views.keys()) #the views member of th is a dictionary of views \n",
    "locked_tokens = th.list_tokens_from_view('locked')\n",
    "\n",
    "print(type(locked_tokens)) #It's not a list!!\n",
    "print(\"There are \"+str(len(locked_tokens))+\" 'locked' tokens\")\n",
    "\n",
    "\n",
    "todo_tokens = th.list_tokens_from_view('todo') \n",
    "# It's not a list because it procedurally pings CouchDB, ~generator\n",
    "#Use the help below to browse how it works!!\n",
    "##help(todo_tokens)\n",
    "\n",
    "print(\"There are \"+str(len(todo_tokens))+\" 'todo' tokens\")\n",
    "print(\"\")\n",
    "print(\"They are:\")\n",
    "for i in todo_tokens:\n",
    "    print(\"CouchDB token keys: \"+str(i.keys()),\"Token ID: \"+i.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set all tokens in a view to a Status, say 'locked'. This automatically locks the tokens!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Lock status of the token: '+str(th.db[man_token_2]['lock'])+\".\")\n",
    "print('Scrub count of the token: '+str(th.db[man_token_2]['scrub_count'])+\".\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('todo')))+\" 'todo' tokens\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('locked')))+\" 'locked' tokens\")\n",
    "print(\"\")\n",
    "print(\"Setting status to locked for all todo tokens\")\n",
    "th.set_view_to_status(view_name='todo',status='locked') #Sets all todo tokens to \"locked\"\n",
    "\n",
    "todo_tokens = th.list_tokens_from_view('todo') \n",
    "print(\"\")\n",
    "\n",
    "print(\"There are \"+str(len(todo_tokens))+\" 'todo' tokens\")\n",
    "### No more todo tokens!\n",
    "\n",
    "\n",
    "locked_tokens = th.list_tokens_from_view('locked')\n",
    "print(\"There are \"+str(len(locked_tokens))+\" 'locked' tokens\")\n",
    "##Now they're all locked!\n",
    "\n",
    "print('Lock status of the token: '+str(th.db[man_token_2]['lock'])+\".\")\n",
    "#You can reset all tokens from a view back to 'todo'. This increments the scrub_count field\n",
    "\n",
    "\n",
    "resetted_tokens=th.reset_tokens('locked')\n",
    "print(\"\")\n",
    "print(\"Resetting the locked tokens\")\n",
    "print('Scrub count of the token: '+str(th.db[man_token_2]['scrub_count'])+\".\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('todo')))+\" 'todo' tokens\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('locked')))+\" 'locked' tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can create your own view. Views collect tokens that satisfy a certain boolean expression (where the token is referenced as 'doc'\n",
    "\n",
    "For example: \n",
    "\n",
    "The todo view satsifies: `'doc.lock ==  0 && doc.done == 0 '` \n",
    "\n",
    "The locked view satisfies: `'doc.lock > 0 && doc.done == 0 '`\n",
    "\n",
    "The done view satsifies: `'doc.status == \"done\" '`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th.add_view(v_name=\"demo_view\",cond='doc.manual_int == 0 ') #Only one of our tokens has manual_int==0\n",
    "print(th.views['demo_view']) #new view is here!\n",
    "\n",
    "assert(len(th.list_tokens_from_view('demo_view'))==1)\n",
    "print(\"There is \"+str(len(th.list_tokens_from_view('demo_view')))+\" tokens in the demo_view\")\n",
    "\n",
    "#Creating 2 more tokens for this view. If append isn't changed, the id is the same, so\n",
    "#new tokens won't be created! But you can imagine a loop will make creation easy right?\n",
    "_ = th.create_token(keys=manual_keys, \n",
    "                            append=\"manual_with_attach_1\",  \n",
    "                            attach=[open(test_srm_file),'srm_at_token_create.txt']) \n",
    "_ = th.create_token(keys=manual_keys, \n",
    "                            append=\"manual_with_attach_2\",\n",
    "                            attach=[open(test_srm_file),'srm_at_token_create.txt'])\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('demo_view')))+\" tokens in the demo_view\")\n",
    "assert(len(th.list_tokens_from_view('demo_view'))==3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can delete all tokens in this view easily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th.delete_tokens('demo_view')\n",
    "assert(len(th.list_tokens_from_view('demo_view'))==0)\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('demo_view')))+\" tokens in the demo_view\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the login node, you sholdn't lock tokens, that's responsibility of the launcher script. After the jobs finish, you can iterate over the 'error' view and reset the tokens if you wish. This makes re-running failed jobs easy, You just have to re-submit the jdl to the Workload Manager!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) The automatic way!\n",
    "\n",
    "\n",
    "When you need to create tokens in bulk, you can do so using a .yaml file and a python dictionary.\n",
    "\n",
    "Now introducing Token Sets: Just an easy way to create tokens from a dictionary using a yaml file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts=Token.TokenSet(th=th) #You need a Token_handler object to create tokensets \n",
    "                         #(TokenHandler manages the authentification, views and token_type selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
