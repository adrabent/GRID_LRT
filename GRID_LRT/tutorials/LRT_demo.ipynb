{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID_LRT Testbed Notebook\n",
    "\n",
    "## 1. Setting up the Environment\n",
    "\n",
    "The GRID LOFAR TOOLS have several infrastructure requirements. They are as follows:\n",
    "\n",
    "1. ASTRON LOFAR staging credentials\n",
    "2. PiCaS database access\n",
    "3. Valid GRID proxy\n",
    "\n",
    "\n",
    "Here, we'll test that all of the above are enabled and work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/apmechev/software/lib/python2.6/site-packages/GRID_LRT-0.2-py2.6.egg/GRID_LRT/__init__.pyc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import GRID_LRT\n",
    "print(GRID_LRT.__file__)\n",
    "import subprocess\n",
    "from GRID_LRT.get_picas_credentials import picas_cred\n",
    "from GRID_LRT.Staging import stage_all_LTA\n",
    "from GRID_LRT.Staging import state_all\n",
    "from GRID_LRT.Staging import stager_access\n",
    "from GRID_LRT.Staging.srmlist import srmlist\n",
    "from GRID_LRT import Token\n",
    "pc=picas_cred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give a confirmation of that your LOFAR ASTRON credentials were properly read:\n",
    "\n",
    "`2017-12-04 17:15:29.097902 stager_access: Parsing user credentials from /home/apmechev/.awe/Environment.cfg\n",
    "2017-12-04 17:15:29.097973 stager_access: Creating proxy`\n",
    "\n",
    "Next, we check that your PiCaS User and Database are set properly. You can also verify your password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apmechev\n",
      "sksp_unittest\n"
     ]
    }
   ],
   "source": [
    "print(pc.user)\n",
    "print(pc.database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the test srm.txt to show off our staging chops:\n",
    "\n",
    "Stage the test srm.txt file. You'll get a StageID that you can use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB101_uv.dppp.MS_acbb43a6.tar', 'srm://srm.grid.sara.nl:8443/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB102_uv.dppp.MS_69304702.tar']\n",
      "files are on SARA\n",
      "Setting up 51 srms to stage\n",
      "staged with stageID  18534\n",
      "18534\n"
     ]
    }
   ],
   "source": [
    "test_srm_file='/home/apmechev/t/GRID_LRT/GRID_LRT/tests/srm_50_sara.txt'\n",
    "\n",
    "os.path.exists(test_srm_file)\n",
    "with open(test_srm_file,'r') as f:\n",
    "    file_contents = f.read()\n",
    "    print(file_contents.split()[0:3]) \n",
    "stageID=stage_all_LTA.main(test_srm_file) # NOTE! You (oll get two emails every time you do this!\n",
    "print(stageID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now re-run the cell below to check the current status of your staging request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new\n",
      "{'18530': {'Status': 'in progress', 'File count': '1', 'User id': '2331', 'Location': 'fz-juelich', 'Files done': '0', 'Flagged abort': 'false', 'Percent done': '0'}, '18534': {'Status': 'new', 'File count': '51', 'User id': '2331', 'Location': 'sara', 'Files done': '0', 'Flagged abort': 'false', 'Percent done': '0'}, '18529': {'Status': 'in progress', 'File count': '1', 'User id': '2331', 'Location': 'fz-juelich', 'Files done': '0', 'Flagged abort': 'false', 'Percent done': '0'}}\n"
     ]
    }
   ],
   "source": [
    "print(stage_all_LTA.get_stage_status(stageID)) #crashes (py2.7?)\n",
    "#The code below can also show you a more detailed status\n",
    "statuses=stager_access.get_progress()\n",
    "\n",
    "print(statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "statuses=stage_all_LTA.get_stage_status(stageID)\n",
    "## When the staging completes, your stageID magically disappears from the database\n",
    "# Neat, huh?\n",
    "if not statuses:\n",
    "    print(\"Staging status no longer in LTA Database\") #This happens because bad programming\n",
    "else:\n",
    "    print(\"Staging request \"+str(stageID)+\" has status: \"+str(statuses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the status of the srms two different ways (with srmls and with gfal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/apmechev/software/lib/python2.6/site-packages/GRID_LRT-0.2-py2.6.egg/GRID_LRT/Staging/state_all.py\n",
      "files are on SARA\n",
      "0/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB100_uv.dppp.MS_3d78b8f1.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "1/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB101_uv.dppp.MS_acbb43a6.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "2/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB102_uv.dppp.MS_69304702.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "3/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB103_uv.dppp.MS_a47a629e.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "4/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB104_uv.dppp.MS_2da7d035.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "5/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB105_uv.dppp.MS_7ad084dd.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "6/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB106_uv.dppp.MS_9c68322c.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "7/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB107_uv.dppp.MS_fd8aee0a.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "8/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB108_uv.dppp.MS_71343baa.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "9/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB109_uv.dppp.MS_74a09e93.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "10/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB110_uv.dppp.MS_4e823dc4.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "11/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB111_uv.dppp.MS_4df8f01f.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "12/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB112_uv.dppp.MS_578ec0cc.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "13/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB113_uv.dppp.MS_5153acbe.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "14/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB114_uv.dppp.MS_a7308002.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "15/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB115_uv.dppp.MS_5d55c402.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "16/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB116_uv.dppp.MS_eefee06b.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "17/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB117_uv.dppp.MS_c3512dfc.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "18/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB118_uv.dppp.MS_8a3a07db.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "19/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB119_uv.dppp.MS_87856a8f.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "20/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB120_uv.dppp.MS_41cdf876.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "21/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB121_uv.dppp.MS_51422fb4.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "22/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB122_uv.dppp.MS_725aa5e4.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "23/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB123_uv.dppp.MS_325bb43f.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "24/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB124_uv.dppp.MS_8b0e62f6.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "25/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB125_uv.dppp.MS_f3acd02d.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "26/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB126_uv.dppp.MS_40a68703.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "27/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB127_uv.dppp.MS_a1e25fba.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "28/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB128_uv.dppp.MS_21dda42b.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "29/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB129_uv.dppp.MS_ca83e7c8.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "30/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB130_uv.dppp.MS_ae9fdea1.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "31/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB131_uv.dppp.MS_bb1a4f54.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "32/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB132_uv.dppp.MS_314aff50.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "33/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB133_uv.dppp.MS_cce8f7ec.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "34/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB134_uv.dppp.MS_2034f17e.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "35/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB135_uv.dppp.MS_b100487b.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "36/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB136_uv.dppp.MS_04288d08.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "37/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB137_uv.dppp.MS_df9b4ff7.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "38/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB138_uv.dppp.MS_b5809b4f.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "39/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB139_uv.dppp.MS_b491a936.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "40/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB140_uv.dppp.MS_a99ed735.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "41/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB141_uv.dppp.MS_568c1e9a.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "42/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB142_uv.dppp.MS_2a25f26f.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "43/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB143_uv.dppp.MS_11c5b025.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "44/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB144_uv.dppp.MS_ffadacf3.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "45/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB145_uv.dppp.MS_8a0a0359.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "46/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB146_uv.dppp.MS_9f12d505.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "47/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB147_uv.dppp.MS_6a966762.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "48/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB148_uv.dppp.MS_5d73b756.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "49/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB149_uv.dppp.MS_5418b56d.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "50/pnfs/grid.sara.nl/data/lofar/ops/projects/lc2_038/229507/L229507_SB150_uv.dppp.MS_f6fc7fc5.tar \u001b[32mONLINE_AND_NEARLINE\u001b[0m\n",
      "files are on SARA\n"
     ]
    }
   ],
   "source": [
    "print(state_all.__file__)\n",
    "staged_status = state_all.main(test_srm_file) #Only works for Sara and Poznan files!\n",
    "\n",
    "#You can also supress the printing of statuses\n",
    "staged_status1 = state_all.main(test_srm_file, printout=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Srmlist()\n",
    "\n",
    "A dedicated class exists to handle lists of srmfiles. This class is a child of the python 'list' class and thus has all the capabilites of a list with some bells and whistles. \n",
    "\n",
    "It contains as properties the OBSID and LTA location of the files. \n",
    "\n",
    "Additionally, it can create generators that convert the srm:// links to gsiftp:// links, as well as staging links (Ones that can be fed into the state_all.py script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_list=srmlist() #Empty list of srms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokens! \n",
    "### 3. a) The manual way\n",
    "\n",
    "Next we'll interface with PiCaS and start making tokens for our Observation:\n",
    "\n",
    "here we need a string to link all the tokens in one Observation. We'll use the string 'demo_'+username in the sksp_dev database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uname = os.environ['USER']\n",
    "th = Token.Token_Handler(t_type=\"jupyter_demo_\"+uname, uname=pc.user, pwd=pc.password, dbn='sksp_dev')\n",
    "\n",
    "#Create the overview_view (has the number of todo, done, error, running, [...] tokens)\n",
    "th.add_overview_view()\n",
    "\n",
    "#Add the satus views (By default 'todo', 'locked', 'done', 'error')\n",
    "th.add_status_views()\n",
    "\n",
    "#Manually create a token:\n",
    "manual_keys = {'manual_key':'manual_value','manual_int':1024}\n",
    "man_token_1 = th.create_token(keys=manual_keys, append=\"manual\") #will return the id of the manual token\n",
    "print('manual_token_ID = ' + man_token_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually create a Token with an automatic attachment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manual_keys = {'manual_key':'manual_value','manual_int':0}\n",
    "man_token_2 = th.create_token(keys=manual_keys, \n",
    "                            append=\"manual_with_attach\",\n",
    "                            attach=[open(test_srm_file),'srm_at_token_create.txt']) \n",
    "\n",
    "##We can also attach files after the token's been created:\n",
    "th.add_attachment(man_token_2, open(test_srm_file), 'srm_added_later.txt')\n",
    "\n",
    "#Double check that both files were attached. Returns a list of filenames:\n",
    "man_2_attachies = th.list_attachments(man_token_2)\n",
    "print(\"The two attached files are: \"+str(man_2_attachies))\n",
    "\n",
    "# We can also of course download attachments:\n",
    "saved_attach=th.get_attachment(man_token_2,man_2_attachies[0],savename=man_2_attachies[0])\n",
    "print(\"\")\n",
    "print('The attachemnt '+str(man_2_attachies[0])+\" was saved at \"+saved_attach)\n",
    "\n",
    "assert(os.path.exists(saved_attach))\n",
    "os.remove(saved_attach)\n",
    "assert(not os.path.exists(saved_attach))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also list the views and the tokens from each view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(th.views.keys()) #the views member of th is a dictionary of views \n",
    "locked_tokens = th.list_tokens_from_view('locked')\n",
    "\n",
    "print(type(locked_tokens)) #It's not a list!!\n",
    "print(\"There are \"+str(len(locked_tokens))+\" 'locked' tokens\")\n",
    "\n",
    "\n",
    "todo_tokens = th.list_tokens_from_view('todo') \n",
    "# It's not a list because it procedurally pings CouchDB, ~generator\n",
    "#Use the help below to browse how it works!!\n",
    "##help(todo_tokens)\n",
    "\n",
    "print(\"There are \"+str(len(todo_tokens))+\" 'todo' tokens\")\n",
    "print(\"\")\n",
    "print(\"They are:\")\n",
    "for i in todo_tokens:\n",
    "    print(\"CouchDB token keys: \"+str(i.keys()),\"Token ID: \"+i.id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set all tokens in a view to a Status, say 'locked'. This automatically locks the tokens!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Lock status of the token: '+str(th.db[man_token_2]['lock'])+\".\")\n",
    "print('Scrub count of the token: '+str(th.db[man_token_2]['scrub_count'])+\".\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('todo')))+\" 'todo' tokens\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('locked')))+\" 'locked' tokens\")\n",
    "print(\"\")\n",
    "print(\"Setting status to locked for all todo tokens\")\n",
    "th.set_view_to_status(view_name='todo',status='locked') #Sets all todo tokens to \"locked\"\n",
    "\n",
    "todo_tokens = th.list_tokens_from_view('todo') \n",
    "print(\"\")\n",
    "\n",
    "print(\"There are \"+str(len(todo_tokens))+\" 'todo' tokens\")\n",
    "### No more todo tokens!\n",
    "\n",
    "\n",
    "locked_tokens = th.list_tokens_from_view('locked')\n",
    "print(\"There are \"+str(len(locked_tokens))+\" 'locked' tokens\")\n",
    "##Now they're all locked!\n",
    "\n",
    "print('Lock status of the token: '+str(th.db[man_token_2]['lock'])+\".\")\n",
    "#You can reset all tokens from a view back to 'todo'. This increments the scrub_count field\n",
    "\n",
    "\n",
    "resetted_tokens=th.reset_tokens('locked')\n",
    "print(\"\")\n",
    "print(\"Resetting the locked tokens\")\n",
    "print('Scrub count of the token: '+str(th.db[man_token_2]['scrub_count'])+\".\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('todo')))+\" 'todo' tokens\")\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('locked')))+\" 'locked' tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can create your own view. Views collect tokens that satisfy a certain boolean expression (where the token is referenced as 'doc'\n",
    "\n",
    "For example: \n",
    "\n",
    "The todo view satsifies: `'doc.lock ==  0 && doc.done == 0 '` \n",
    "\n",
    "The locked view satisfies: `'doc.lock > 0 && doc.done == 0 '`\n",
    "\n",
    "The done view satsifies: `'doc.status == \"done\" '`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th.add_view(v_name=\"demo_view\",cond='doc.manual_int == 0 ') #Only one of our tokens has manual_int==0\n",
    "print(th.views['demo_view']) #new view is here!\n",
    "\n",
    "assert(len(th.list_tokens_from_view('demo_view'))==1)\n",
    "print(\"There is \"+str(len(th.list_tokens_from_view('demo_view')))+\" tokens in the demo_view\")\n",
    "\n",
    "#Creating 2 more tokens for this view. If append isn't changed, the id is the same, so\n",
    "#new tokens won't be created! But you can imagine a loop will make creation easy right?\n",
    "_ = th.create_token(keys=manual_keys, \n",
    "                            append=\"manual_with_attach_1\",  \n",
    "                            attach=[open(test_srm_file),'srm_at_token_create.txt']) \n",
    "_ = th.create_token(keys=manual_keys, \n",
    "                            append=\"manual_with_attach_2\",\n",
    "                            attach=[open(test_srm_file),'srm_at_token_create.txt'])\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('demo_view')))+\" tokens in the demo_view\")\n",
    "assert(len(th.list_tokens_from_view('demo_view'))==3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can delete all tokens in this view easily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th.delete_tokens('demo_view')\n",
    "assert(len(th.list_tokens_from_view('demo_view'))==0)\n",
    "print(\"There are \"+str(len(th.list_tokens_from_view('demo_view')))+\" tokens in the demo_view\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the login node, you sholdn't lock tokens, that's responsibility of the launcher script. After the jobs finish, you can iterate over the 'error' view and reset the tokens if you wish. This makes re-running failed jobs easy, You just have to re-submit the jdl to the Workload Manager!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) The automatic way!\n",
    "\n",
    "\n",
    "When you need to create tokens in bulk, you can do so using a .yaml file and a python dictionary.\n",
    "\n",
    "Now introducing Token Sets: Just an easy way to create tokens from a dictionary using a yaml file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts=Token.TokenSet(th=th) #You need a Token_handler object to create tokensets \n",
    "                         #(TokenHandler manages the authentification, views and token_type selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
